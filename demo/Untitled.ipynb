{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the package\n",
    "import stanza\n",
    "\n",
    "#https://stanfordnlp.github.io/stanza/client_regex.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-03 14:32:14 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | ewt       |\n",
      "| pos       | ewt       |\n",
      "| lemma     | ewt       |\n",
      "| depparse  | ewt       |\n",
      "| sentiment | sstplus   |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2020-11-03 14:32:14 INFO: Use device: cpu\n",
      "2020-11-03 14:32:14 INFO: Loading: tokenize\n",
      "2020-11-03 14:32:14 INFO: Loading: pos\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building an English pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-03 14:32:15 INFO: Loading: lemma\n",
      "2020-11-03 14:32:15 INFO: Loading: depparse\n",
      "2020-11-03 14:32:16 INFO: Loading: sentiment\n",
      "2020-11-03 14:32:17 INFO: Loading: ner\n",
      "2020-11-03 14:32:18 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Build an English pipeline, with all processors by default\n",
    "print(\"Building an English pipeline...\")\n",
    "en_nlp = stanza.Pipeline('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing English text\n",
    "en_doc = en_nlp(\"Oscar will give us the number of flowers to plant, and we will put the pots against the wall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanza.server import CoreNLPClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get noun phrases with tregex\n",
    "def noun_phrases(_client, _text, _annotators=None):\n",
    "    pattern ='{word:wrote} >nsubj {}=subject >obj {}=object'\n",
    "    matches = _client.tregex(_text,pattern,annotators=_annotators)\n",
    "    print(\"\\n\".join([\"\\t\"+sentence[match_id]['spanString'] for sentence in matches['sentences'] for match_id in sentence]))\n",
    "\n",
    "# English example\n",
    "    with CoreNLPClient(timeout=30000, memory='16G') as client:\n",
    "        englishText = \"Albert Einstein was a German-born theoretical physicist. He developed the theory of relativity.\"\n",
    "        print('---')\n",
    "        print(englishText)\n",
    "        noun_phrases(client,englishText,_annotators=\"tokenize,ssplit,pos,lemma,parse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tregex() missing 1 required positional argument: 'pattern'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-304394f08cee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnoun_phrases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCoreNLPClient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-ebe473045b4d>\u001b[0m in \u001b[0;36mnoun_phrases\u001b[0;34m(_client, _text, _annotators)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnoun_phrases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_annotators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'{word:wrote} >nsubj {}=subject >obj {}=object'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtregex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_text\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mannotators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_annotators\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmatch_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'spanString'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentences'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmatch_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: tregex() missing 1 required positional argument: 'pattern'"
     ]
    }
   ],
   "source": [
    "noun_phrases(CoreNLPClient,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=(\"Oscar will give us the number of flowers to plant, and we will put the pots against the wall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-03 14:57:42 INFO: Writing properties to tmp file: corenlp_server-ca4f9207567f4fec.props\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Please install CoreNLP by running `stanza.install_corenlp()`. If you have installed it, please define $CORENLP_HOME to be location of your CoreNLP distribution or pass in a classpath parameter.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-0b54d24d6fb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mannotators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokenize'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ssplit'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'pos'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'lemma'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ner'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'parse'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'depparse'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         memory='16G') as client:\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Use tokensregex patterns to find who wrote a sentence.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/KAIU/stanza/stanza/server/client.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, start_server, endpoint, timeout, threads, annotators, output_format, properties, stdout, stderr, memory, be_quiet, max_char_length, preload, classpath, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mhost\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"localhost\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"If starting a server, endpoint must be localhost\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m             \u001b[0mclasspath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresolve_classpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasspath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m             \u001b[0mstart_cmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"java -Xmx{memory} -cp '{classpath}'  edu.stanford.nlp.pipeline.StanfordCoreNLPServer \"\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m                         \u001b[0;34mf\"-port {port} -timeout {timeout} -threads {threads} -maxCharLength {max_char_length} \"\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/KAIU/stanza/stanza/server/client.py\u001b[0m in \u001b[0;36mresolve_classpath\u001b[0;34m(classpath)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasspath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0;34m\"Please install CoreNLP by running `stanza.install_corenlp()`. If you have installed it, please define \"\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m             \u001b[0;34m\"$CORENLP_HOME to be location of your CoreNLP distribution or pass in a classpath parameter.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mclasspath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasspath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Please install CoreNLP by running `stanza.install_corenlp()`. If you have installed it, please define $CORENLP_HOME to be location of your CoreNLP distribution or pass in a classpath parameter."
     ]
    }
   ],
   "source": [
    "text = \"Chris Manning is a nice person. Chris wrote a simple sentence. He also gives oranges to people.\"\n",
    "with CoreNLPClient(\n",
    "        annotators=['tokenize','ssplit','pos','lemma','ner', 'parse', 'depparse'],\n",
    "        timeout=30000,\n",
    "        memory='16G') as client:\n",
    "\n",
    "    # Use tokensregex patterns to find who wrote a sentence.\n",
    "    pattern = '([ner: PERSON]+) /wrote/ /an?/ []{0,3} /sentence|article/'\n",
    "    matches = client.tokensregex(text, pattern)\n",
    "    # sentences contains a list with matches for each sentence.\n",
    "    print(len(matches[\"sentences\"])) # prints: 3\n",
    "    # length tells you whether or not there are any matches in this\n",
    "    print(matches[\"sentences\"][1][\"length\"]) # prints: 1\n",
    "    # You can access matches like most regex groups.\n",
    "    print(matches[\"sentences\"][1][\"0\"][\"text\"]) # prints: \"Chris wrote a simple sentence\"\n",
    "    print(matches[\"sentences\"][1][\"0\"][\"1\"][\"text\"]) # prints: \"Chris\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
